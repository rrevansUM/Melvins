Gaussian Processes
========================================================
author: Michael Clark, Richard Evans
date: 10/18/2017
width: 1440
height: 1000
font-family: 'Helvetica'

Disclaimer
========================================================

```{r, echo = FALSE, eval = TRUE}
library(MASS)
library(ggplot2)
library(reshape2)
library(magrittr)
```

Code ripped from [Michael Clark's Github Repo](https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/gp%20Examples/gaussianprocessNoisy.R)

I am made this presentation to learn more about making presentations in Rstudio and Gaussian Processes

Rationale
========================================================

'Noisy' gaussian process demo. The matrix labeling is in keeping with Murphy 2012 and Rasmussen and Williams 2006.  See those sources for more detail. Murphy's matlab code can be found here: https://code.google.com/p/pmtk3/, though the relevant files are housed alongside this code.

The goal of this code is to plot samples from the prior and posterior predictive of a gaussian process in which $y=sin(x)+noise$. It will reproduce an example akin to figure 15.3 in Murphy 2012.                                              

Functions
========================================================

the mean function; in this case mean = 0

```{r}
muFn = function(x) {
  x = sapply(x, function(x) x = 0)
  x
}
```

Functions (cont.)
========================================================

The covariance function, Kfn(). Here it is the squared exponential kernel. 
    * l is the horizontal scale
    * sigmaf is the vertical scale
    * sigman the noise. 
    
See ?covSEiso in the gpr package for example, which is also based on Rasmussen and Williams Matlab code (gpml Matlab library)

```{r}
Kfn = function(x, y = NULL, l = 1, sigmaf = 1, sigman = 0.5) {
  dist_ <- as.matrix(dist(x, upper = T, diag = T)^2)
  sqexpo <- exp(-(1 / (2 * l^2)) * dist_)
  if(!is.null(y)) {
    sigmaf * sqexpo + sigman * diag(length(x))    
  } else {
    sigmaf * sqexpo
  }
}
```

Preliminaries
========================================================

```{r}
l <- 1           # for l, sigmaf, sigman, see note at covariance function
sigmaf <- 1      
sigman <- .25 
keps <- 1e-8     # see note at Kstarstar
nprior <- 5      # number of prior draws
npostpred <- 3   # number of posterior predictive draws
```

Prior Plot
========================================================

```{r, echo = FALSE, eval = TRUE}
xg1 <- seq(-5, 5, .2)
yg1 <- mvrnorm(nprior, 
               mu = muFn(xg1), 
               Sigma = Kfn(xg1, l = l, sigmaf = sigmaf, sigman = sigman)) 

gdat <- melt(data.frame(x = xg1, y = t(yg1), sd = apply(yg1, 2, sd)), 
             id = c('x', 'sd'))
```

```{r, echo = FALSE, fig.width = 8, fig.height = 5, dpi = 300, out.width = "1920px", out.height = "1080px"}
g1 <- gdat %>%
  ggplot(aes(x = x, y = value)) %>%
  add(geom_line(aes(group = variable, color = variable), alpha = .5)) %>%
  add(theme_minimal())
g1
```

generate noisy training data
========================================================

```{r}
Xtrain <- 15 * (runif(20) - .5)  
nTrain <- length(Xtrain)
ytrain <- sin(Xtrain) + rnorm(n = nTrain, sd = 0.1)  # kept sine function for comparison to noise free result

Xtest <- seq(-7.5, 7.5, length = 200)
nTest <- length(Xtest)

plot(Xtrain, ytrain)
```

Generate posterior predictive
========================================================

Create Ky, K*, and K** matrices as defined in the texts

```{r}
Ky <- Kfn(x = Xtrain, y = ytrain, l = l, sigmaf = sigmaf, sigman = sigman)
dim(Ky)
# initial matrix
K_ <- Kfn(c(Xtrain, Xtest), l = l, sigmaf = sigmaf, sigman = sigman)
dim(K_)
# dim = N x N*
Kstar <- K_[1:nTrain, (nTrain + 1):ncol(K_)]
dim(Kstar)
```

Generate posterior predictive (cont.)
========================================================

```{r}
# dim = N* x N
tKstar <- t(Kstar)
dim(tKstar)
# dim = N* x N*; the keps part is for positive definiteness
Kstarstar <- K_[(nTrain+1):nrow(K_), (nTrain+1):ncol(K_)] + keps * diag(nTest)
dim(Kstarstar)
Kyinv <- solve(Ky)
dim(Kyinv)
```

Calculate posterior mean and covariance
========================================================

```{r}
postMu <- muFn(Xtest) + tKstar %*% Kyinv %*% (ytrain - muFn(Xtrain))
head(postMu)

postCov <- Kstarstar - tKstar %*% Kyinv %*% Kstar
dim(postCov)

s2 <- diag(postCov)
# R = chol(postCov)  
# L = t(R)  # L is used in alternative formulation below based on gaussSample.m
```

Generate draws from posterior predictive
========================================================

```{r}
y2 <- data.frame(t(mvrnorm(npostpred, mu = postMu, Sigma = postCov)))
# y2 = data.frame(replicate(npostpred, postMu + L %*% rnorm(postMu))) # alternative
head(y2)
```

Posterior Predictive plot
========================================================

```{r, echo = FALSE, eval = TRUE}
selower <- postMu - 2 * sqrt(s2)
seupper <- postMu + 2 * sqrt(s2)
gdat <- melt(data.frame(x = Xtest, 
                        y = y2, 
                        fmean = postMu,
                        selower = selower,
                        seupper = seupper),
             id = c('x', 'fmean', 'selower', 'seupper'))
```

```{r, echo = FALSE, fig.width = 8, fig.height = 5, dpi = 300, out.width = "1920px", out.height = "1080px"}
g2 <- gdat %>%
  ggplot(aes(x = x, y = value)) %>%
  add(geom_ribbon(aes(ymin = selower, ymax = seupper, group = variable),
                  fill = 'gray90')) %>%
  add(geom_line(aes(group = variable), color = '#FF5500', alpha = 0.5)) %>%
  add(geom_line(aes(group = variable, y = fmean), color = 'navy')) %>%
  add(geom_point(aes(x = Xtrain, y = ytrain), 
                 data = data.frame(Xtrain, ytrain))) %>%
  add(theme_minimal())
g2
```

Posterior Predictive and Prior Distributions
========================================================

```{r, echo = FALSE, fig.width = 12, fig.height = 6, dpi = 300, out.width = "1920px", out.height = "1080px"}
library(gridExtra)
grid.arrange(g1, g2, ncol = 2)
```
